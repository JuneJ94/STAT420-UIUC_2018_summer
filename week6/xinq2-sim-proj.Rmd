---
title: "xinq2-sim-proj"
author: "STAT 420, Summer 2018, Xin Qu"
date: ''
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---
##Simulation Study 1, Significance of Regression
***
**Introduction**

In this study, we will investigate the significane of regression test. We will simulate from the following two different models: 

  1. The **"significant"** model:

$Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i$

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 3$

- $\beta_1 = 1$

- $\beta_2 = 1$

- $\beta_3 = 1$

  2. The **"non-significant"** model:

$Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i$

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 3$

- $\beta_1 = 0$

- $\beta_2 = 0$

- $\beta_3 = 0$

For both models, we will consider a sample size of 25 and three possible levels of noise. That is the three values of $\sigma$. 

- $n = 25$

- $\sigma \in (1, 5, 10)$

We will use simulation to obtain an empirical distribution for each of the following values for each of the three values of $\sigma$, for both models. 

- The **F statistic** for the significance of regression test

- The **p-value** for the significance of regression test

- $R ^ 2$

For each model-$\sigma$ combination, we will use 2500 simulations. For each simulation, we will fit a regression model of the same form used to perform the simulation. We will use the data found in $\text{study_1.csv}$ for the values of the predictors. 

We will discuss the following discussions:

- Do we know the true distribution of any of these values?

- How do the empirical distributions from the simulations compare to the true distributions? (We could consider adding a curve for the true distributions if we know them.)

- How are $R ^ 2$ and $\sigma$ related? Is the relationship the same for the significant and non-significant models?

**Method**

This section should include R code and code comments as needed. At the beginning of this study, we will set a seed equal to my birthday, which is 19890310. In the following parts, model1 refers to **"significant"** model, and model2 refers to **"non-significant"** model. 

```{r}
##set up my birthday
birthday = 19890310
set.seed(birthday)
```

```{r}
##number of simulation for each sigma
num_sim = 2500
```

```{r}
##values of sigma for each model
##values of sigma for model1
beta_0_model1 = 3
beta_1_model1 = 1
beta_2_model1 = 1
beta_3_model1 = 1
##values of sigma for model2
beta_0_model2 = 3
beta_1_model2 = 0
beta_2_model2 = 0
beta_3_model2 = 0

##sample size (refers to n from Instruction section) and levels of noise
sample_size = 25
sigma = c(1, 5, 10)
```

```{r}
##read the data from 'study_1.csv'
sim_data = read.csv('study_1.csv')
```

estimates_model1 to hold variables of model1. F_stat is used as the placeholder vector for the estimates of F statistic, p_val for p-value and R_2 for $R^2$. 

estimates_model2 to hold variables of model2. F_stat is used as the placeholder vector for the estimatse of F statistic, p_val for p-value and R_2for $R^2$. 

```{r}
estimates_model1 = data.frame(
  F_stat = rep(0, num_sim), 
  p_val = rep(0, num_sim), 
  R_2 = rep(0, num_sim)
)

estimates_model2 = data.frame(
  F_stat = rep(0, num_sim), 
  p_val = rep(0, num_sim), 
  R_2 = rep(0, num_sim)
)
```

data_model1 is used to place all values of estimated from model1 under three different values of $\sigma$ and data_model2 is used to place all values of estimated from model2 . 

```{r}
data_model1 = list(beta_1_sim = estimates_model1, 
                   beta_2_sim = estimates_model1, 
                   beta_3_sim = estimates_model1)
data_model2 = list(beta_1_sim = estimates_model2, 
                   beta_2_sim = estimates_model2, 
                   beta_3_sim = estimates_model2)
```
```{r}
##library(broom) ##use glance function to double check
```
Run the simulation for model1(significant model)
```{r}
###run the simulation for each sigma for model1
for (s in 1: length(sigma)) {
  for (i in 1: num_sim) {
    ###simulate data y and save new y into study_1.csv file
    sim_data$y = with(sim_data, beta_0_model1 + beta_1_model1 * x1+ beta_2_model1 * x2 + beta_3_model1 * x3 + rnorm(n = sample_size, mean = 0, sd = sigma[s]))
    
    ##fit model
    fit_model1 = lm(y ~ ., data = sim_data)
    null_model = lm(y ~ 1, data = sim_data)
    ##find three values
    estimates_model1$F_stat[i] = summary(fit_model1)$fstatistic[1]
    estimates_model1$p_val[i] = anova(null_model, fit_model1)$"Pr(>F)"[2] 
    ##estimates_model1$F_stat[i] = glance(fit_model1)$stat
    ##estimates_model1$p_val[i] = glance(fit_model1)$p.val
    ##estimates_model1$R_2[i] = glance(fit_model1)$r.squared
    estimates_model1$R_2[i] = summary(fit_model1)$r.squared
    ##the three variables match the results from glance
  }
  data_model1[[s]] = estimates_model1
}
```
Run the same simulation for model2(non-significant model)
```{r}
for (s in 1: length(sigma)) {
  for (i in 1: num_sim) {
    ###simulate data y and save new y data into study_1.csv file
    sim_data$y = with(sim_data, beta_0_model2 + beta_1_model2 * x1+ beta_2_model2 * x2 + beta_3_model2 * x3 + rnorm(n = sample_size, mean = 0, sd = sigma[s]))
    
    ##fit model
    fit_model2 = lm(y ~ ., data = sim_data)
    null_model2 = lm(y ~ 1, data = sim_data)
    ##find three values
    estimates_model2$F_stat[i] = summary(fit_model2)$fstatistic[1]
    estimates_model2$p_val[i] = anova(null_model2, fit_model2)$"Pr(>F)"[2]
    estimates_model2$R_2[i] = summary(fit_model2)$r.squared
    ##estimates_model2$F_stat[i] = glance(fit_model2)$stat
    ##estimates_model2$p_val[i] = glance(fit_model2)$p.val
    ##estimates_model2$R_2[i] = glance(fit_model2)$r.squared
     ##the three variables match the results from glance
  }
  data_model2[[s]] = estimates_model2
}
```

**Results**

We will graph the results of the simulations and fit the curve of the true distribution for comparison if we know the true distribution. At first, we'll list true distribution of the three estimates if we know the true distribution:

F-statistic follows the F distribution as follows:

$F = \frac{{\sum_{i = 1} ^ {n} (\hat{y}_{1i} - \bar{y}) ^ 2 / (p - 1)}} {{\sum_{i = 1} ^ {n} (y_i -\hat{y}_{1i}) ^ 2 / (n - p)}}$

$F-statistic \sim F\left(p - 1, n - p \right)$  

where $p - 1$ is the quantity of predictors, $n$ is the quantity of observed data. In this case, $p - 1 = 3$, $n = 25$.

The true distribution of $R^2$ can be derived from the followings: 

$F = \frac{(n-p)}{(p - 1)}\frac{R^2}{1 - R^2}$

$R^2 = \frac{(p - 1) F} {(n - p) + (p - 1)F}$ 

$R^2 \sim Beta{(\frac{p - 1}{2}, \frac{n - p}{2})}$, where $n$, $p$ is the same as the one of the F distribution of F statistic. $R^2$ follows a Beta Distribution. 

As for the true distribution of p-value, we could only know the true distribution of p-value under the null hypothesis and will be derived from the followings: 

test statistic $T$ follows the F distribution $F(t) = Pr(T < t)$ for all t, $P = F(T)$, then

$Pr(P < p) = Pr(F(T) < p) = Pr(T < F^{-1}(p)) = F(F^{-1}(p)) = p$, so we could conclude that the true distribution of p-value under the null hypothesis is uniform distribution on $[0, 1]$. 

But we don't know the true distribution of p-value under the full hypothesis, we could calculate p-value as $P(F(p-1, n - p) > F)$. 

Graphs of F statistic empirical distribution for each $\sigma$ for model1(significant model) and model2(non-significant model). For better comparison, we could set up both xlim and ylim on plots to show full true distribuiton and full empirical distribution.

```{r}
par(mfrow = c(2, 3))
###in order to show the true distribution with the F statistic empirical distribution for model1(significant model) when sigma = 1, set up both ylim and xlim. 
hist(data_model1[[1]]$F_stat, prob = TRUE, xlab = "F-stat, model1", main = "sigma = 1", border = "dodgerblue", breaks = 100, xlim = c(0, 250), ylim = c(0, 0.08))
##set ylim different from other plots to show both empirical and true distribution more clearly.
x = seq(0, 100, length = 100)
curve(df(x, df1 = 4 - 1, df2 = 25 - 4), add = TRUE, col = 'orange', lwd = 2)
hist(data_model1[[2]]$F_stat, prob = TRUE, xlab = "F-stat, model1", main = "sigma = 5", border = "dodgerblue", breaks = 100, ylim = c(0, 0.8))
curve(df(x, df1 = 4 - 1, df2 = 25 - 4), add = TRUE, col = 'orange', lwd = 2)
hist(data_model1[[3]]$F_stat, prob = TRUE, xlab = "F-stat, model1", main = "sigma = 10", border = "dodgerblue", breaks = 100, ylim = c(0, 0.8))
curve(df(x, df1 = 4 - 1, df2 = 25 - 4), add = TRUE, col = 'orange', lwd = 2)
##model2
hist(data_model2[[1]]$F_stat, prob = TRUE, xlab = "F-stat, model2", main = "sigma = 1", border = "dodgerblue", breaks = 100, ylim = c(0, 0.8))
x = seq(0, 100, length = 100)
curve(df(x, df1 = 4 - 1, df2 = 25 - 4), add = TRUE, col = 'orange', lwd = 2)
hist(data_model2[[2]]$F_stat, prob = TRUE, xlab = "F-stat, model2", main = "sigma = 5", border = "dodgerblue", breaks = 100, ylim = c(0, 0.8))
curve(df(x, df1 = 4 - 1, df2 = 25 - 4), add = TRUE, col = 'orange', lwd = 2)
hist(data_model2[[3]]$F_stat, prob = TRUE, xlab = "F-stat, model2", main = "sigma = 10", border = "dodgerblue", breaks = 100, ylim = c(0, 0.8))
curve(df(x, df1 = 4 - 1, df2 = 25 - 4), add = TRUE, col = 'orange', lwd = 2)
```

Graphs of p-value empirical distribution for each $\sigma$ for for model1(significant model) and model2(non-significant model). The weighted orange horizontal lines represent for uniform distributions. 

```{r}
par(mfrow = c(2, 3))
x = seq(0, 1, length = 100)
hist(data_model1[[1]]$p_val, prob = TRUE, xlab = "p-value, model1", main = "sigma = 1", border = "dodgerblue", breaks = 100)
curve(dunif(x, min = 0, max = 1), add = TRUE, col = 'orange', lwd = 2)
hist(data_model1[[2]]$p_val, prob = TRUE, xlab = "p-value, model1", main = "sigma = 5", border = "dodgerblue", breaks = 100)
curve(dunif(x, min = 0, max = 1), add = TRUE, col = 'orange', lwd = 2)
hist(data_model1[[3]]$p_val, prob = TRUE, xlab = "p-value, model1", main = "sigma = 10", border = "dodgerblue", breaks = 100)
curve(dunif(x, min = 0, max = 1), add = TRUE, col = 'orange', lwd = 2)
###model2
x = seq(0, 1, length = 100)
hist(data_model2[[1]]$p_val, prob = TRUE, xlab = "p-value, model2", main = "sigma = 1", border = "dodgerblue", breaks = 100)
curve(dunif(x, min = 0, max = 1), add = TRUE, col = 'orange', lwd = 2)
hist(data_model2[[2]]$p_val, prob = TRUE, xlab = "p-value, model2", main = "sigma = 5", border = "dodgerblue", breaks = 100)
curve(dunif(x, min = 0, max = 1), add = TRUE, col = 'orange', lwd = 2)
hist(data_model2[[3]]$p_val, prob = TRUE, xlab = "p-value, model2", main = "sigma = 10", border = "dodgerblue", breaks = 100, ylim = c(0, 1.5))
curve(dunif(x, min = 0, max = 1), add = TRUE, col = 'orange', lwd = 2)
```

Graphs of $R^2$ empirical distribution for each $\sigma$ for model1(significant model) and model2(non-significant model). For better comparison, we could set up both xlim and ylim on plots to show full true distribuiton and full empirical distribution.

```{r}
par(mfrow = c(2, 3))
###in order to show the true distribution with empirical distribution for model1 with sigma = 1, set up xlim in range[0, 1]
hist(data_model1[[1]]$R_2, prob = TRUE, xlab = expression(R^2), main = "sigma = 1, model1", border = "dodgerblue", breaks =100, xlim = c(0, 1))
x = seq(0, 1, length = 100)
curve(dbeta(x, shape1 = (4 - 1)/ 2, shape2 = (25 - 4) / 2), add = TRUE, col = 'orange', lwd = 2)
hist(data_model1[[2]]$R_2, prob = TRUE, xlab = expression(R^2), main = "sigma = 5, model1", border = "dodgerblue", breaks = 100, ylim = c(0, 6))
curve(dbeta(x, shape1 = (4 - 1)/ 2, shape2 = (25 - 4) / 2), add = TRUE, col = 'orange', lwd = 2)
hist(data_model1[[3]]$R_2, prob = TRUE, xlab = expression(R^2), main = "sigma = 10, model1", border = "dodgerblue", breaks = 100, ylim = c(0, 6))
curve(dbeta(x, shape1 = (4 - 1)/ 2, shape2 = (25 - 4) / 2), add = TRUE, col = 'orange', lwd = 2)
hist(data_model2[[1]]$R_2, prob = TRUE, xlab = expression(R^2), main = "sigma = 1, model2", border = "dodgerblue", breaks =100)
curve(dbeta(x, shape1 = (4 - 1)/ 2, shape2 = (25 - 4) / 2), add = TRUE, col = 'orange', lwd = 2)
hist(data_model2[[2]]$R_2, prob = TRUE, xlab = expression(R^2), main = "sigma = 5, model2", border = "dodgerblue", breaks = 100)
curve(dbeta(x, shape1 = (4 - 1)/ 2, shape2 = (25 - 4) / 2), add = TRUE, col = 'orange', lwd = 2)
hist(data_model2[[3]]$R_2, prob = TRUE, xlab = expression(R^2), main = "sigma = 10, model2", border = "dodgerblue", breaks = 100, ylim = c(0, 6))
curve(dbeta(x, shape1 = (4 - 1)/ 2, shape2 = (25 - 4) / 2), add = TRUE, col = 'orange', lwd = 2)
```

The following are tables of means and standard deviations of three estimates under three values of $\sigma$ for model1(significant model) and model2(non-significant model). These tables could be used to find out the relationship between each variable. 

Table of mean for model1(significant model)
```{r}
F_stat_mean = c(mean(data_model1[[1]]$F_stat), 
                mean(data_model1[[2]]$F_stat), 
                mean(data_model1[[3]]$F_stat))

p_val_mean = c(mean(data_model1[[1]]$p_val), 
               mean(data_model1[[2]]$p_val),
               mean(data_model1[[3]]$p_val))

R_2_mean = c(mean(data_model1[[1]]$R_2), 
             mean(data_model1[[2]]$R_2),
             mean(data_model1[[3]]$R_2))

###table of mean for model1
sigma_values = c("`sigma = 1`", "`sigma = 5`", "`sigma = 10`")
result1 = data.frame(sigma_values, F_stat_mean, p_val_mean, R_2_mean)
colnames(result1) = c('sigma', 'F statistic', 'p-value','R_2')
knitr::kable(result1)
```

Table of mean for model2(non-significant model)
```{r}
F_stat_mean = c(mean(data_model2[[1]]$F_stat), 
                mean(data_model2[[2]]$F_stat), 
                mean(data_model2[[3]]$F_stat))

p_val_mean = c(mean(data_model2[[1]]$p_val), 
               mean(data_model2[[2]]$p_val),
               mean(data_model2[[3]]$p_val))

R_2_mean = c(mean(data_model2[[1]]$R_2), 
             mean(data_model2[[2]]$R_2),
             mean(data_model2[[3]]$R_2))

###table of mean for model2
sigma_values = c("`sigma = 1`", "`sigma = 5`", "`sigma = 10`")
result2 = data.frame(sigma_values, F_stat_mean, p_val_mean, R_2_mean)
colnames(result2) = c('sigma', 'F statistic', 'p-value','R_2')
knitr::kable(result2)
```

Table of standard deviation for model1(significant model)
```{r}
F_stat_sd = c(sd(data_model1[[1]]$F_stat), 
                sd(data_model1[[2]]$F_stat), 
                sd(data_model1[[3]]$F_stat))

p_val_sd = c(sd(data_model1[[1]]$p_val), 
               sd(data_model1[[2]]$p_val),
               sd(data_model1[[3]]$p_val))

R_2_sd = c(sd(data_model1[[1]]$R_2), 
             sd(data_model1[[2]]$R_2),
             sd(data_model1[[3]]$R_2))

###table of sd for model1
sigma_values = c("`sigma = 1`", "`sigma = 5`", "`sigma = 10`")
result1_sd = data.frame(sigma_values, F_stat_sd, p_val_sd, R_2_sd)
colnames(result1_sd) = c('sigma', 'F statistic', 'p-value','R_2')
knitr::kable(result1_sd)
```

Table of standard deviation for model2(non-significant model)
```{r}
F_stat_sd = c(sd(data_model2[[1]]$F_stat), 
                sd(data_model2[[2]]$F_stat), 
                sd(data_model2[[3]]$F_stat))

p_val_sd = c(sd(data_model2[[1]]$p_val), 
               sd(data_model2[[2]]$p_val),
               sd(data_model2[[3]]$p_val))

R_2_sd = c(sd(data_model2[[1]]$R_2), 
             sd(data_model2[[2]]$R_2),
             sd(data_model2[[3]]$R_2))

###table of sd for model2
sigma_values = c("`sigma = 1`", "`sigma = 5`", "`sigma = 10`")
result2_sd = data.frame(sigma_values, F_stat_sd, p_val_sd, R_2_sd)
colnames(result2_sd) = c('sigma', 'F statistic', 'p-value','R_2')
knitr::kable(result2_sd)
```

**Discussion**

As we mentioned in Results section, we could know the true distribution for F statistic, which follows F distribution, $F-statistic \sim F\left(p - 1, n - p \right)$ and the true distribution for $R^2$ is Beta distribution, $R^2 \sim Beta{(\frac{p - 1}{2}, \frac{n - p}{2})}$, where $p - 1$ is the quantity of predictors, $n$ is the quantity of observed data. And the true distribution for p-value under the null hypothesis is uniform distribution in $[0, 1]$. 

From Results section, we don't know the true distribution of p-value under the full hypothesis, but p-value is calculated as $P(F(p-1, n - p) > F)$. The p-values under null hypothesis are uniformly distributed. In this study, model2(non-significant model) is much similar to null hypothesis, which only $\beta_0 \neq 0$ and $\beta_{1,2,3} = 0$. It means that the true distribution of p-value from model2(non-significant model) should be a uniform distribution. The definition of model1(significant), which is $\beta_{0, 1, 2, 3} \neq 0$ is much similar to the full hypothesis (at least one of $\beta_i \neq 0$). 

The true distribution of F statistic is F distribution, $F-statistic \sim F\left(p - 1, n - p \right)$. From the plots of F statistic for the 2 models, we could tell the empirical distribution of F statistic for model2(non-significant model) fits true distribution well under each value of $\sigma$. But the empirical distribution of F-statistic for model1(significant model) does not fit the true distribution well, especially when $\sigma$ is small, in this case when $\sigma = 1$ and $\sigma = 5$. When $\sigma = 1$, the empirical distribution of F statistic for model1(significant model) does not fit the true distribution at all. As for $\sigma = 10$ for model1(significant model), the empirical distribution of F statistic fits the true distribution much better than the other two values of $\sigma$. The empirical distribution of F statistic for model1(significant model) fits true distribution (F distribution) better by increasing the value of $\sigma$. 

The true distribution of p-value under null hypothesis is uniform distribution. From the plots of p-value distributions for the 2 models, we could tell the empirical distribution of p-value for model2(non-significant model) fits the uniform distribution (true distribution) well under each value of $\sigma$. But the empirical distribution of p-value of model1(significant model) does not fit the uniform distribution well, especially when $\sigma = 1$ and $\sigma = 5$. When $\sigma = 1$, the empirical distribution of p-value for model1(significant model) does not fit the uniform distribution at all.When $\sigma = 10$, the empirical distribution of p-value for model1(significant model) fits uniform distribution much better than the other two values of $\sigma$. The empirical distributions of p-value for model1(significant model) fits true distribution (uniform distribution) better by increasing the value of $\sigma$. 

The true distribution of $R^2$ is Beta distribution, $R^2 \sim Beta{(\frac{p - 1}{2}, \frac{n - p}{2})}$. From the plots of $R^2$ for the 2 models, we could tell the empirical distribution of $R^2$ for model2(non-significant model) fits true distribution well under each value of $\sigma$. But the empirical distribution of $R^2$ for model1(significant model) does not fit the true distribution well, especially when $\sigma$ is small, in this case when $\sigma = 1$ and $\sigma = 5$. When $\sigma = 1$, the empirical distribution of $R^2$ for model1(significant model) does not fit the true distribution at all. As for $\sigma = 10$ for model1(significant model), the empirical distribution of $R^2$ fits the true distribution much better than the other two values of $\sigma$. The empirical distributions of $R^2$ for model1(significant model) fits true distribution (Beta distribution) better by increasing the value of $\sigma$. 

We could conclude that the empirical distributions of F statistic, p-value and $R^2$ of non-significant model (model2 in this study) fit the corresponding true distribution very well under each value of $\sigma$. The empirical distributions of F statistic, p-value and $R^2$ of significant model (model1 in this study) fit the true distribution better by increasing the value of $\sigma$ and when the value of $\sigma$ is small, the empirical distributions do not fit the true distributions at all. 

Then we will discuss the relationship between $R^2$ and $\sigma$ for the two models. 

From the mean table for model1 (significant model), we can tell that the mean of $R^2$ decreases by increasing value of $\sigma$. But the relationship is not the same for model2(non-significant model). As for model2(non-significant model), the mean value of $R^2$ does not change a lot as the value of $\sigma$ increases, which can also tell from the small values of standard deviation of $R^2$. And the mean of $R^2$ under each $\sigma$ for model2(non-significant model) is smaller than the mean of $R^2$ under each $\sigma$ for model1(significant model). 

From the above tables, including mean tables and standard deviation tables for both model1(significant model) and model2(non-significant model), we could tell the mean values of F statistic and $R^2$ for model1(significant model) decrease by increasing value of $\sigma$, and the mean value of p-value increases by increasing value of $\sigma$. But it's not the same as model2(non-significant model). From the tables for model2(non-significant model), the mean values of F statistic, p-value and $R^2$ do not change a lot by increasing or decreasing value of $\sigma$. 

##Simulation Study 2, Using RMSE for selection?
***
**Introduction**

In this study, we will investigate how well the procedure of using Test RMSE to find the "best" model work. Since splitting the data is random, we don’t expect it to work correctly each time. But averaged over many attempts, we should expect it to select the appropriate model if we could get lucky.

We will simulate from the model

$Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + \beta_5 x_{i5} + \beta_6 x_{xi6} + \epsilon_i$, where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 0$

- $\beta_1 = 5$

- $\beta_2 = -4$

- $\beta_3 = 1.6$

- $\beta_4 = -1.1$

- $\beta_5 = 0.7$

- $\beta_6 = 0.3$

We will consider a sample size of 500 and three possible levels of noise. That is, three values of $\sigma$.

- $n = 500$

- $\sigma \in (1, 2, 4)$

For the values of the predictors, we will use the data found in $\text{study_2.csv}$. Each time of simulation, we will split the data randomly into train and test sets of equal sizes (250 observations for training, 250 observations for testing). In each simulation, we will fit **nine** models with the following forms:

- y ~ x1

- y ~ x1 + x2

- y ~ x1 + x2 + x3

- y ~ x1 + x2 + x3 + x4

- y ~ x1 + x2 + x3 + x4 + x5, the correct form of the model

- y ~ x1 + x2 + x3 + x4 + x5 + x6

- y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7

- y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8

- y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9

For each model, we will calculate Train and Test RMSE.

  $\text{RMSE}(\text{model, data}) = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}$

We will repeat this process with 1000 simulations for each of the three values of $\sigma$. For each value of $\sigma$, we will graph how average Train RMSE and average Test RMSE change as a function of model size. We will also show the number of times the model of each size was chosen for each value of $\sigma$.

By the end of this study, we will see if the method always selects the correct model, or if, on average, it selects the correct model. We will also see how the level of noise affect the results.

**Methods**

Set the seed to my birthday.

```{r}
birthday = 19890310
set.seed(birthday)
```

```{r}
##qty of simulation
num_sim = 1000
##qty of models
num_model = 9
##create placeholders for both train and test RMSE under each model
RMSE = list(
  train_RMSE = matrix(c(rep(0, num_model * num_sim)), ncol = num_model, nrow = num_sim),
  test_RMSE = matrix(c(rep(0, num_model * num_sim)), ncol = num_model, nrow = num_sim)
)
```
Create a list (all_data) to hold all the simulation results
```{r}
all_data = list(sigma_1_sim = RMSE,
                sigma_2_sim = RMSE, 
                sigma_3_sim = RMSE)

##beta values of model
beta_0 = 0
beta_1 = 5
beta_2 = -4
beta_3 = 1.6
beta_4 = -1.1
beta_5 = 0.7
beta_6 = 0.3
##values of sigma
sigma = c(1, 2, 4)
##import data from study_2.csv
sim_data = read.csv('study_2.csv')
```
```{r}
##function to calculate RMSE
rmse = function(true_val, predict_val) {
  sqrt(mean((true_val - predict_val) ^ 2))
}

##run simulation

for (s in 1: length(sigma)) {
  for (i in 1: num_sim) {
    ##get y values for sim_data
    sim_data$y = with(sim_data,
                        beta_0 + beta_1 * x1 + beta_2 * x2 + beta_3 * x3 + beta_4 * x4 + beta_5 * x5 + beta_6 * x6 + rnorm(n = 500, mean = 0, sd = sigma[s]))
    
        
    ##split data into test and train randomly
    train_index = sample(1: nrow(sim_data), 250)
    train_data = sim_data[train_index, ]
    test_data = sim_data[-train_index, ]
    
    ##fit models
    fit_1 = lm(y ~ x1, data = train_data)
    fit_2 = lm(y ~ x1 + x2, data = train_data)
    fit_3 = lm(y ~ x1 + x2 + x3, data = train_data)
    fit_4 = lm(y ~ x1 + x2 + x3 + x4, data = train_data)
    fit_5 = lm(y ~ x1 + x2 + x3 + x4 + x5, data = train_data)
    fit_6 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6, data = train_data)
    fit_7 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7, data = train_data)
    fit_8 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = train_data)
    fit_9 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9, data = train_data)
    
    ##get RMSE for train data
    train_errors = c(
      rmse(train_data$y, predict(fit_1, train_data)),
      rmse(train_data$y, predict(fit_2, train_data)),
      rmse(train_data$y, predict(fit_3, train_data)),
      rmse(train_data$y, predict(fit_4, train_data)),
      rmse(train_data$y, predict(fit_5, train_data)),
      rmse(train_data$y, predict(fit_6, train_data)),
      rmse(train_data$y, predict(fit_7, train_data)),
      rmse(train_data$y, predict(fit_8, train_data)),
      rmse(train_data$y, predict(fit_9, train_data))
    )
    ##get RMSE for test data
    
    test_errors = c(
      rmse(test_data$y, predict(fit_1, test_data)),
      rmse(test_data$y, predict(fit_2, test_data)),
      rmse(test_data$y, predict(fit_3, test_data)),
      rmse(test_data$y, predict(fit_4, test_data)),
      rmse(test_data$y, predict(fit_5, test_data)),
      rmse(test_data$y, predict(fit_6, test_data)),
      rmse(test_data$y, predict(fit_7, test_data)),
      rmse(test_data$y, predict(fit_8, test_data)),
      rmse(test_data$y, predict(fit_9, test_data))
    )
    
    RMSE$train_RMSE[i, ] = train_errors
    RMSE$test_RMSE[i, ] = test_errors
  }
  
  all_data[[s]] = RMSE
}
```

**Results**

We will graph the average of Train RMSE and Test RMSE for each $\sigma$ as a function of model size. 

```{r}
##graphs both RMSE with sigma = 1
train_means = colMeans(all_data[[1]]$train_RMSE)
test_means = colMeans(all_data[[1]]$test_RMSE)
ylim = range(train_means, test_means)
plot(test_means, pch = 1, col = "blue", ylim = ylim, xlab = "Model Size", ylab = "Average RMSE", main = "Sigma = 1", lwd = 2, type = 'o')
points(train_means, pch = 5, col = "orange", lwd = 2, type = 'o')
legend('topright', legend=c("Test RMSE", "Train RMSE"), col=c("blue", "orange"), pch = c(1, 5), cex = 1, lwd = 2)
```

```{r}
##graphs both RMSE with sigma = 2
train_means = colMeans(all_data[[2]]$train_RMSE)
test_means = colMeans(all_data[[2]]$test_RMSE)
ylim = range(train_means, test_means)
plot(test_means, pch = 1, col = "blue", ylim = ylim, xlab = "Model Size", ylab = "Average RMSE", main = "Sigma = 2", lwd = 2, type = 'o')
points(train_means, pch = 5, col = "orange", lwd = 2, type = 'o')
legend('topright', legend=c("Test RMSE", "Train RMSE"), col=c("blue", "orange"), pch = c(1, 5), cex = 1, lwd = 2)
```


```{r}
##graphs both RMSE with sigma = 4
train_means = colMeans(all_data[[3]]$train_RMSE)
test_means = colMeans(all_data[[3]]$test_RMSE)
ylim = range(train_means, test_means)
plot(test_means, pch = 1, col = "blue", ylim = ylim, xlab = "Model Size", ylab = "Average RMSE", main = "Sigma = 4", lwd = 2, type = 'o')
points(train_means, pch = 5, col = "orange", lwd = 2, type = 'o')
legend('topright', legend=c("Test RMSE", "Train RMSE"), col=c("blue", "orange"), pch = c(1, 5), cex= 1, lwd = 2)
```

In order to get a better comparison, we will create a table with the average of train RMSE and test RMSE for each model size under each $\sigma$. 

```{r}
table_data = data.frame(colMeans(all_data[[1]]$train_RMSE), 
                        colMeans(all_data[[2]]$train_RMSE), 
                        colMeans(all_data[[3]]$train_RMSE), 
                        colMeans(all_data[[1]]$test_RMSE), 
                        colMeans(all_data[[2]]$test_RMSE), 
                        colMeans(all_data[[3]]$test_RMSE))

size = c('1', "2", "3", "4", "5", "6", "7", "8", '9')
result = data.frame(size, table_data[1], table_data[2], table_data[3], table_data[4], table_data[5], table_data[6])
colnames(result) = c('model size', 'train sigma = 1', 'train sigma = 2', 'train sigma = 4', 'test sigma = 1', 'test sigma = 2', 'test sigam = 4')
knitr::kable(result)
```

Next, we will show the number of times the model of each size is chosen with smallest test RMSE for each value of $\sigma$. 

Table of number of times the model of each size is chosen as the "best" model for each value of $\sigma$
```{r}
##times_data is a placeholder of all the data under each sigma. times is a placeholder of data under each model
times = matrix(c(rep(0, num_model * num_sim)), 
               ncol = num_model, 
               nrow = num_sim)
times_data = list(sigma_1_t = times, 
                  sigma_2_t = times,
                  sigma_3_t = times)
```
```{r}
###run the simualtion to select the best model and count the number of times
for (s in 1: length(sigma)) {
  for (i in 1: num_sim) {
    
    row = all_data[[s]]$test_RMSE[i, ]
    
    minimum = min(row) ### find the minimum test RMSE
    
    times[i, ] = as.numeric(row == minimum)
  }
  times_data[[s]] = times
}
```
```{r}
###nums hold the total qty of being selected as the best model
nums = data.frame(colSums(times_data[[1]]), 
                  colSums(times_data[[2]]), 
                  colSums(times_data[[3]]))
size = c('1', "2", "3", "4", "5", "6", "7", "8", '9')
result_time = data.frame(size, nums[1], nums[2], nums[3])
colnames(result_time) = c('model size', 'sigma = 1', 'sigma = 2', 'sigma = 4')
knitr::kable(result_time)
```

From the above table, we could tell the model y ~ x1 + x2 + x3 + x4 + x5 + x6 is chosen by the greatest chance than other models under $\sigma = 1$ and $\sigma = 2$. The "best" model under $\sigma = 4$ is model 3. 

In order to get better comparison, we will plot the times of each model slected as the "best" model under each $\sigma$ and set up the same ylim in range [0, 550]. The highlighted "darkgreen" color bar shows the "best" model with the maximum selected times. 

Plot of times under each value of $\sigma$.
```{r}
par(mfrow = c(1, 3))
##sigma = 1
cols = ifelse(nums[[1]] == max(nums[[1]]), 'darkgreen', 'darkgrey')
barplot(nums[[1]], xlab = "Model size n", ylab = "Times of model being selected", main = 'sigma = 1', col = cols, border = 'black', axes = TRUE,names.arg = seq(1, 9), ylim = c(0, 550))
##sigma = 2
cols = ifelse(nums[[2]] == max(nums[[2]]), 'darkgreen', 'darkgrey')
barplot(nums[[2]], xlab = "Model size n", ylab = "Times of model being selected", main = 'sigma = 2', col = cols, border = 'black', axes = TRUE,names.arg = seq(1, 9), ylim = c(0, 550))
##sigma = 4
cols = ifelse(nums[[3]] == max(nums[[3]]), 'darkgreen', 'darkgrey')
barplot(nums[[3]], xlab = "Model size n", ylab = "Times of model being selected", main = 'sigma = 4', col = cols, axes = TRUE,names.arg = seq(1, 9), ylim = c(0, 550))
```

**Discussion**

From the above three plots of train RMSE and test RMSE, we could tell that average values of train RMSE and test RMSE "match" pretty well with small value of $\sigma$. By "match", it means the vertical distance between each pair of average of train RMSE and average of test RMSE under each given model size $n$ is small. The vertical distances between average of train RMSE and average of test RMSE increase by increasing the value of $\sigma$ under a given model size $n$ and increase by increasing the model size $n$ under a given value of $\sigma$. We could also tell that the average values of both train RMSE and test RMSE trend to decrease by increasing the model size $n$ under each given value of $\sigma$. It's not easy to find out the lowest (minimum) value of test RMSE from the plot, but we will find out the minimum value of test RMSE from the table of means of train RMSE and test RMSE. We could also tell that the largest (highest) value of average RMSE for both train RMSE and test RMSE are from model 1. 

Then we will talk more about plots of test RMSE and train RMSE under each value of $\sigma$:

- $\sigma = 1$: Each average value of test RMSE and train RMSE are pretty close to each other by increasing the model size $n$. Both two average values trend to decrease by increasing model size $n$. 

- $\sigma = 2$: the vertical distances between average values of test RMSE and train RMSE increase by increasing the model size $n$ and the average values of train RMSE and test RMSE trend to decrease by increasing model size $n$. 

- $\sigma = 4$: the vertical distances between average values of test RMSE and train RMSE is the largest among three values of $\sigma$ under each value of model size $n$. When model size $n$ is getting bigger, the vertical distance between each pair of average of train RMSE and average of test RMSE increases. From this plot, we could tell clearly that the value of average test RMSE decreases by increasing the value of model size $n$ at first before it reaches the minimum value, then the average value of test RMSE starts to increase a little bit by increasing the value of model size $n$.

From the table of means of train RMSE and test RMSE, we can tell that average of train RMSE decreases with increasing model size $n$ under a given value of $\sigma$ and increases by increasing value of $\sigma$ under each given value of model size $n$. But as for the average of test RMSE, the smallest value is from model 6 under each value of $\sigma$. By increasing the value of model size $n$, the average test RMSE decreases till model size $n = 6$, then starts to increase. It shows that the average of test RMSE increases by increasing the value of $\sigma$ with each given model size $n$. And model6 y ~ x1 + x2 + x3 + x4 + x5 + x6 has the smallest test RMSE for each value of $\sigma$, which indicates that model6 y ~ x1 + x2 + x3 + x4 + x5 + x6 will be selected as the "best" model.

From the table of times and three plots of times, it's easy to see that the method does not always select the best model (Model 5 for this simulation study is the correct model as described in Introduction section). From graphs of test RMSE and train RMSE and table of train RMSE and test RMSE, model 6(y ~ x1 + x2 + x3 + x4 + x5 + x6) is selected as the best model under each value of $\sigma$ since it always has the smallest test RMSE. But from the table of times, model 6 was chosen as the "best model" for $\sigma = 1$ and $\sigma = 2$ by greatest chance (around 50% and 40%) and model 3 was chosen as the "best" model for $\sigma = 4$ with probibality 0.245. Another thing we can tell is that the levels of noise have some effects on both train RMSE and test RMSE. It means that the average values of test RMSE increase by increasing the value of $\sigma$ ($\sigma$ is the level of noise) under each given model size $n$, so do the average values of train RMSE. The average values of train RMSE decrease clearly by increasing the model size $n$ under each given value of $\sigma$, but the trend is not clear for the average values of test RMSE since the smallest value of average test RMSE decreases by increasing model size $n$ till the model size $n$ reaches to 6 then the average value of test RMSE starts to increase by increasing model size $n$. 

From the table of times and three plots of times, even we run the simulation for 1000 times for each $\sigma$, we could tell that model6 is "always" (not all the time) selected as the best model since it has the largest the sum of number of times each model when $\sigma = 1$ and $\sigma = 2$ and model 3 is selected as the best model when $\sigma = 4$ by greatest chance. We are told that the correct model is as following: model 5 y ~ x1 + x2 + x3 + x4 + x5. But the RMSE method does not alawys select the correct model, not even at average. The chance of selecting the correct model (model 5) as the best model for this simulation study is pretty small, which are only 9/1000 (0.009) for $\sigma = 1$, 75/1000 (0.075) for $\sigma = 2$ and 116/1000 (0.116) for $\sigma = 4$. We could also tell from the table of times and three plots of times that the level of noise, $\sigma$ has some effects on results. When $\sigma = 4$, after 1000 simualtions, the best model is model 3 with 245 selected times, which is not the same model as selected best model under $\sigma = 1$ and $\sigma = 2$. We could conclude that the best mode selected by RMSE method may vary under different levels of noise $\sigma$.

Another thing we could tell from both plots and table related to times is that the distribution of times of being selected as the "best" model trend to be equal by increasing value of $\sigma$. Model 6 is chosen as the "best" model by chance around 0.5 under $\sigma = 1$ and by chance around 0.4 under $\sigma = 2$, the chance of other models being selected as the "best" model is much less than 0.5 and 0.4. But when $\sigma$ increases to 4, even model 3 is chosen as the "best"" model by chance of 0.245 (245 / 1000), the chance of model 6 being selected as the "best" model is much close to 0.245, which is 0.191 (191/1000). And the chance of model 4 (182/1000) is pretty close to chance of model 6 (191/1000). 

##Simulation Study 3, Power

**Introduction**

In this study, we will investigate the **power** of the significance of regression test for simple linear regression

$H_0: \beta_{1} = 0 \ \text{vs} \ H_1: \beta_{1} \neq 0$

Recall, we had defined the $\text{significance}$ level, $\alpha$, to be the probability of a Type I error. 

$\alpha = P[\text{Reject }H_0 \mid H_0 \text{ True}] = P[\text{Type I Error}]$

Similarly, the probability of a Type II error is often denoted using $\beta$

$\beta = P[\text{Fail to Reject }H_0 \mid H_1 \text{ True}] = P[\text{Type II Error}]$

$\text{Power}$ is the probablity of rejecting the null hypothesis when the null is not true, that is, the alternative is true and $\beta_1$ is non-zero. 

$Power = 1 - \beta = P [\text{Reject } H_0 \mid H_1 \text{ True}]$
 
Essentially, power is the probablity that a signal of a particular strength will be detected. Many things affect the power of a test. In this case, some of those are:

- Sample Size, $n$

- Signal Strength, $\beta_1$

- Noise level, $\sigma$

- Significance level, $\alpha$

We will investigate the first three of the above factors. We will simulate from the model

$Y_i = \beta_0 + \beta_1 x_i + \epsilon_i$ where $\epsilon_i \sim N(0, \sigma^2)$

We will let $\beta_0 = 0$ for simpicity, thus $\beta_1$ is essentially controlling the amount of "signal". We will then consider different signals, noises, and sample sizes:

- $\beta_1 \in (-2, -1.9, -1.8,\ldots, -0.1, 0, 0.1, 0.2, 0.3, \ldots, 1.9, 2)$

- $\sigma \in (1, 2, 4)$

- $n \in (10, 20, 30)$

We will hold the significance level constant at $\alpha = 0.05$. We will use the following code to generate the predictor values, $x$, values for different sample sizes. "x_values = seq(0, 5, length = n)"

For each possible $\beta_1$ and $\sigma$ combination, simulate from the true model at least 1000 times. Each time we will perform the significance of the regression test. To estimate the power with these simulations, and some $\alpha$, use

$\hat{\text{Power}} = \hat{P} [\text{Reject } H_0 \mid H_1 \text { True}] = \frac{\text{# Tests Rejected}}{\text{# Simulations}}$

We will create three plots, one for each value of $\sigma$. Within each of these plots, add a “power curve” for each value of $n$ that shows how power is affected by signal strength $\beta_1$. We hope by the end of this study to show how $n$, $\beta_1$ and $\sigma$ affect power and whether are 1000 simulations sufficient. 

**Methods**

We will start by setting the seed to birthday and set up some constant variables.

```{r}
birthday = 19890310
set.seed(birthday)
```
```{r}
beta_1 = seq(-2, 2, by = 0.1)
simga = c(1, 2, 4)
n = c(10, 20, 30)
alpha = 0.05
num_sim = 1000
```

reject is a placeholder matrix to store all the results (0 or 1, 1 means p-value < $\alpha$) of significance of the regression test at $\alpha = 0.05$. powers is another placeholder matrix to store all power values for a value of $n$ vs a value of $\beta_1$.

```{r}
##create the matrix to store rejection results
reject = matrix(c(rep(0, length(n) * num_sim)),
                ncol = length(n), 
                nrow = num_sim)
##create matrix to store power for n vs beta_1

powers = matrix(c(rep(0, length(n) * length(beta_1))), 
                ncol = length(n), 
                nrow = length(beta_1))
##create a list to store all the data for each sigma
all_sim3 = list(sigma_1 = powers, 
           sigma_2 = powers, 
           sigma_3 = powers)
```
Run the simulation test

```{r}
for (s in 1: length(sigma)) {
  for (b in 1: length(beta_1)) {
    for (a in 1: length(n)) {
      ###generate the predictor values x and sim_data
        x_values = seq(0, 5, length = n[a])
        sim_data = data.frame(y = rep(0, length(x_values)), x = x_values)
        for (i in 1: num_sim) {
          ###simulate y
          sim_data$y = with(sim_data, beta_1[b] * x + rnorm(n = length(x_values), mean = 0, sd = sigma[s]))
          
          ##fit model
          fit = lm(y ~ x, data = sim_data)
          
          ##run significance test
          p_val = summary(fit)$coefficients['x', 'Pr(>|t|)']
          reject[i, a] = as.numeric(p_val < alpha) ### 1: p_val < alpha
        }
        powers[b, a] = mean(reject[, a])
    }
  }
  all_sim3[[s]] = powers
}
```

**Results**

In order to see how power is affected by $n$, $\beta_1$ and $\sigma$, we will plot power vs $\beta_1$ and add three power curves for each value of $n$. The x-axis stands for the value of $\beta_1$.

Plot for $\sigma = 1$.
```{r}
###plot power curve for sigma = 1
plot(all_sim3[[1]][, 1] ~ seq(-2, 2, by = 0.1), type = 'l', lty = 2, lwd = 3, xlab = expression(beta[1]), ylab = 'power', col = 'blue', main = 'sigma = 1')
par(new=TRUE)
plot(all_sim3[[1]][, 2], type = "l", lty = 2, ann = FALSE, axes = FALSE, lwd = 3, col = 'red')
par(new=TRUE)
plot(all_sim3[[1]][, 3], type = "l", lty = 2, ann = FALSE, axes = FALSE, lwd = 3, col = 'orange')
legend('bottomright', legend=c("n = 10", "n = 20", "n = 30"), col=c("blue", "red", "orange"), lty=2, cex=1)
```

Plot for $\sigma = 2$. 
```{r}
###plot power curve for sigma = 2
plot(all_sim3[[2]][, 1]~ seq(-2, 2, by = 0.1), type = 'l', lty = 2, lwd = 3, xlab = expression(beta[1]), ylab = 'power', col = 'blue', main = 'sigma = 2')
par(new=TRUE)
plot(all_sim3[[2]][, 2], type = "l", lty = 2, ann = FALSE, axes = FALSE, lwd = 3, col = 'red')
par(new=TRUE)
plot(all_sim3[[2]][, 3], type = "l", lty = 2, ann = FALSE, axes = FALSE, lwd = 3, col = 'orange')
legend('bottomright', legend=c("n = 10", "n = 20", "n = 30"), col=c("blue", "red", "orange"), lty=2, cex=1)
```

Plot for $\sigma = 4$.

In order to compare with the above 2 plots, make the y-axis range plot for $\sigma = 4$ the same as the two above plots (This may change the layout of power curves). 

```{r}
###plot power curve for sigma = 4
plot(all_sim3[[3]][, 1]~ seq(-2, 2, by = 0.1), type = 'l', lty = 2, lwd = 3, xlab = expression(beta[1]), ylab = 'power', col = 'blue', main = 'sigma = 4', ylim = c(0, 1))
par(new=TRUE)
plot(all_sim3[[3]][, 2], type = "l", lty = 2, ann = FALSE, axes = FALSE, lwd = 3, col = 'red')
par(new=TRUE)
plot(all_sim3[[3]][, 3], type = "l", lty = 2, ann = FALSE, axes = FALSE, lwd = 3, col = 'orange')
legend('bottomright', legend=c("n = 10", "n = 20", "n = 30"), col=c("blue", "red", "orange"), lty=2, cex=1)
```

In order to show how $n$, $\beta_1$ and $\sigma$ affect power, we will create the plot showing the relationship between sample size $n$ and average power under each value of $\sigma$. 

```{r}
plot(colMeans(all_sim3[[1]]), xlab = 'sample size', ylab = 'average of power', main = 'sample size vs average power', pch = 1, lwd = 2, col = 'darkgrey', ylim = c(0, 1), xaxt = 'n', type = 'o')
axis(side = 1, at = seq(1, 3, by = 1), labels = c(10, 20, 30))
par(new = TRUE)
plot(colMeans(all_sim3[[2]]), xlab = 'sample size', ylab = 'average of power', main = 'sample size vs average power', pch = 0, lwd = 2, col = 'darkgreen', ylim = c(0, 1), xaxt = 'n', type = 'o')
par(new = TRUE)
plot(colMeans(all_sim3[[3]]), xlab = 'sample size', ylab = 'average of power', main = 'sample size vs average power', pch = 2, lwd = 2, col = 'pink', ylim = c(0, 1), xaxt = 'n', type = 'o')
legend('bottomright', legend=c("sigma = 1", "sigma = 2", "sigma = 4"), col=c("darkgrey", "darkgreen", "pink"), pch = c(1, 0, 2), cex=1, lwd = 2)
```

**Discussion**

From the above three plots, we can tell that the power curve become wider by increasing the values of $\sigma$. It means that the power curve converges faster (by 'faster' here, it means within larger symmetric range of $\beta_1$, will be explained in the following detailed discussion) to 1 by decreasing value of $\sigma$, level of noise. Since test value (t value) follows t distribution with $(n - p)$ degrees of freedom, in this case $p = 2$, it means that the power curves are supposed to be symmetric with the center $\beta_1 = 0$. With a given level of noise, the value of $\sigma$, the power curve become narrower by increasing the sample size n, which means the power curve converges faster (by 'faster' here, it means within larger symmetric range of $\beta_1$) to 1 by increasing sample size $n$. But from plot of  $\sigma = 4$, it's not easy to tell the range of $\beta_1$ whether power curves converge to 1, expecially when $n = 10$ and the power curves are the widest among the three values of $\sigma$. For all the power curves under each value of $\sigma$, the lowest values of power happens when the value of $\beta_1$ around 0. Within the symmetric range of $\beta_1$, when $\beta_1 > 0$, power value increases by inceasing the value of $\beta_1$ and will converge to a certian value (in this study, most converge to value of 1.0); when $\beta_1 < 0$, power value increases by decreasing the value of $\beta_1$ and will converge to the same value (in this study, most converge to value of 1.0). We could also tell from the three plots that the power curves under smaller values of sample size $n$ and larger value of $\sigma$ are not as smooth as the power curves under larger values of sample size $n$ and smaller value of $\sigma$.  

Let's discuss more details of the power curves with different values of $\sigma_1$. 

- $\sigma = 1$, when $n = 10$, the blue power curve converges close to 1.0 when the value of $\beta_1$ is less than -1 (estimated from plot), which means the value of $\beta_1$ is in estimated range [-2, -1.1]. Symmetrically, blue power curve converges close to 1 when the value of $\beta_1$ is in estimated range [1.1, 2]. The symmetric range of $\beta_1$ for power curve converging to 1 is from [-2, -1.1] and [1.1, 2]. When $n = 20$, in the symmetric range [-2, -0.7] (estimated) and [0.7, 2] (estimated) of $\beta_1$, the red power cure converges to 1. The boundaries of symmetric increase from -1.1 to -0.7 of end boundary for the left range and decrease from 1.1 to 0.7 of start boundary. This means the symmetric range increase by increasing the sample size $n$. When $n = 30$, in the symmetric range [-2, -0.5] (estimated) and [0.5, 2] (estimated) of $\beta_1$, the orange power cure converges to 1. The symmetric range of $\beta_1$ increases by increasing sample size $n$. And power curves become narrower by increasing the sample size $n$. 

- $\sigma = 2$, when $n = 10$, the blue power curve converges close to 1.0 when the value of $\beta_1$ is at the beginging of list of $\beta_1$, the value of $\beta_1$ is less than -1.9 (estimated from plot). Symmetrically, blue power curve converges close to 1 when the value of $\beta_1$ is larger than 1.9 (estimated from plot). The symmetric range of $\beta_1$ value for blue power curve converging to 1 is [-2, -1.9] and [1.9, 2]. For the red power curve ($n = 20$), the symmetric range is [-2, -1.3] (estimated from plot) and [1.3, 2] (estimated from plot). For the orange power curve ($n = 30$), the symmetric range is [-2, -1.1] (estimated from plot) and [1.1, 2] (estimated from plot). The blue power curve and red power curve are not as smooth as the orange power curve. The symmetric range increases by increasing sample size $n$. And power curves become narrower by increasing the sample size $n$. 

- $\sigma = 4$, it's hard to tell the value of $\beta_1$ when power curves start to converge, especially for the case $n = 10$ since there is no clear horizontal line trend. It seems that when $\beta_1$ in the very beginning and the very end of the range, rend and orange power curves converge to around 1.0 when $n = 20$ and $n = 30$. The blue power curve ($n = 10$) seems to converge to 0.6 when $\beta_1 = -2$ and $\beta_1 = 2$. We also noticed that the power curve under $n = 10$ is not as smooth as other two sample sizes and when $\beta_1 = -2$ and $\beta_1 = 2$ the blue power curve converges to 0.6, rather than 1.0. And power curves become narrower by increasing the sample size $n$. The blue power curve is not as smooth as red power curve and red power curve is not as smooth as orange power curve. 

From the last plot, we could tell that the average power value trends to increase by increasing the sample size $n$ under a given value of $\sigma$. And when value of $\sigma$ is small, the mean value of power under each smaple size is close to each other and the mean value of power is close to 0.8 (under $\sigma = 1$). The mean power value trend to decrease by increasing the value of $\sigma$ under a given sample size $n$. 

With 1000 simulations with different levels of $n$, $\beta_1$ and $\sigma$, we are able to see how these three factors can affect power. By changing the number of simulations and observing the results, we could say that 1000 simulations are suffcient at minimum to help see how $n$, $\beta_1$ and $\sigma$ affect power. 